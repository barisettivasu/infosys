for trouble shooting of kube api-server then goto the lab killerkoda api-server crash lab then u can able to understand easily.
vi /etc/kubernetes/manifests/  --> all manifest files shows here. like kube api-server.yml,etcd.yml,scheduler.yml,controller.yml. these are all master node default
pods which are running in kube-system namespace. for checking
kubectl get pods -n kube-system.
if something wrong with any default pods then u can see yml fikes or pods describing way.

logs checking for pods and containers in k8s:
-----------------------------------------------
/var/log/pods
/var/log/containers
crictl ps + crictl logs
docker ps + docker logs (in case when Docker is used)
kubelet logs: /var/log/syslog or journalctl

Solution:
-------------
# always make a backup !
cp /etc/kubernetes/manifests/kube-apiserver.yaml ~/kube-apiserver.yaml.ori

# make the change
vim /etc/kubernetes/manifests/kube-apiserver.yaml

# wait till container restarts
watch crictl ps

# check for apiserver pod
k -n kube-system get pod

Apiserver is not coming back, we messed up!:
------------------------------------------------
# check pod logs
cat /var/log/pods/kube-system_kube-apiserver-controlplane_a3a455d471f833137588e71658e739da/kube-apiserver/X.log
> 2022-01-26T10:41:12.401641185Z stderr F Error: unknown flag: --this-is-very-wrong

Now undo the change and continue:
-----------------------------------
# smart people use a backup
cp ~/kube-apiserver.yaml.ori /etc/kubernetes/manifests/kube-apiserver.yaml

switch to namespace:
---------------------
kubectl config set-context $(kubectl config current-context) --namespace=default
-----------------------
Lab: 0 Create kubeadm cluster 

 Lab: 1
Task: 1 Create pod using kubectl command

kubectl run mypod --image=nginx:latest
#list pods
kubectl get pods

#ssh/login to that pod

kubectl exec -it mypod -- bash 
#to comeout of pod run exit command.
exit
# to Read pod's log
kubectl logs mypod
#Describe pod to see it's configuration
kubectl describe pod mypod

Task: 2
# multiple container in pods 

vi multi-container.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    app: myapp
  name: multi-con-pod
spec:
  containers:
  - name: con1   ### first container 
    image: nginx:latest
    ports:
    - containerPort: 80
  - name: con2    ## second container 
    image: tomcat
    ports:
    - containerPort: 8080

# to save file :wq

create pod by using below command 
kubectl create -f multi-container.yaml

a. read logs of specific contaienr
 kubectl logs {pod-name} -c {container-name}
 kubectl logs multi-con-pod -c con2

b. login into specific container
kubectl exec -it {pod-name} -c {container-name} -- bash
kubectl exec -it multi-con-pod -c con1 -- bash

to out of container run exit command 


Lab 2: Services in Kubernetes


Task 1: Create a Pod using a YAML file

kubectl get nodes
kubectl get nodes -o wide

vi httpd-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: httpd-pod
  labels:
    app: myapp
spec:
  containers:
  - name: httpd-container
    image: httpd
    ports:
      - containerPort: 80

# to save file use :wq!


kubectl create -f httpd-pod.yaml
kubectl get pods
kubectl get pods -o wide
kubectl get pod httpd-pod -o yaml
kubectl describe pod httpd-pod 
kubectl get pods --show-labels

Task 2: Setup ClusterIP service

vi httpd-svc.yaml

apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    app: myapp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP

# to save :wq

kubectl apply -f httpd-svc.yaml
kubectl get services
kubectl describe svc httpd-svc
kubectl get ep 
kubectl get svc httpd-svc

Create a pod and login to that pod to execute curl command 

kubectl run testpod --image=httpd:latest --port 80
kubectl exec -it testpod -- bash
apt update && apt install curl -y
curl http://httpd-svc

kubectl get nodes -owide | awk '{print $7}'

ssh -t admin@< replace Node_IP> curl < replace Cluster_IP>:< replace Service_Port>

Task 3: Setup NodePort service

vi httpd-svc.yaml

# Replace the content of httpd-svc.yaml with below content use ESc and :wq! to quit vi

apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    app: myapp
  ports:
  - protocol: TCP
    port: 80
  type: NodePort

# End of file

kubectl apply -f httpd-svc.yaml

kubectl get svc 
kubectl describe svc httpd-svc
kubectl get nodes -owide | grep node | awk '{print $7}'
curl http://<Your NODE_EXTERNAL_IP>:<Your NodePort>


Task 4: Setup LoadBalancer service

vi httpd-svc.yaml

# Replace the content of httpd-svc.yaml with below content use ESc and :wq! to quit vi

apiVersion: v1
kind: Service
metadata:
  name: httpd-svc
spec:
  selector:
    app: myapp
  ports:
  - protocol: TCP
    port: 80
  type: LoadBalancer


# End of file

kubectl apply -f httpd-svc.yaml
kubectl describe svc httpd-svc
curl http://<Your LoadBalancer_Ingress>:<Your Service_Port>

Task 5: Delete and recreate httpd Pod

kubectl get pod httpd-pod -o wide
kubectl describe svc httpd-svc
kubectl delete pod httpd-pod
kubectl describe svc httpd-svc
kubectl create -f httpd-pod.yaml
kubectl get pod httpd-pod -o wide
kubectl describe svc httpd-svc

Task 6: Clean-up

kubectl delete -f httpd-pod.yaml
kubectl delete -f httpd-svc.yaml

Lab: 3 Static Pod

To create static pod use folloing commands on worker or master node whereever 
you want ot place static pod
if you have any pod.yml file then if u place that file in this path then pod automatically creates without create or apply command.
cd /etc/kubernetes/manifests/

create following static-pod.yaml file here 

vi static-pod.yaml


apiVersion: v1
kind: Pod
metadata:
  labels:
    app: myapp
  name: static-pod
spec:
  containers:
  - name: app 
    image: nginx:latest
    ports:
    - containerPort: 80


kubectl create -f static-pod.yaml
kubectl get pods -o wide 

remove resource
kubectl delete -f static-pod.yaml




Lab 4: Kubernetes Network Policy:
-------------------------------------
#restrict connections from container to container here see which container to which container connections needs and which to wchich container i dnt need connection for that we make a network policy

kubectl run backend --image=nginx --labels role=backend --expose --port=80

kubectl get pod
kubectl get svc


kubectl run --rm -it --image=busybox net-policy 
wget -qO- -T3 http://backend
exit

3.Create a network policy which uses labels to deny all ingress traffic

vi np-deny-all.yml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      role: backend
  ingress: []

kubectl create -f np-deny-all.yml
kubectl get networkpolicies

4.Create a new busybox pod again and verify that it cannot access the backend service

kubectl run --rm -it --image=busybox net-policy
wget -qO- -T3 http://backend
it will show timeout
exit



5.Modify the network policy to allow traffic with matching Pod labels 
Inspect the network policy and notice the selectors

vi np-pod-label-allow.yml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      role: backend
  ingress:
  - from:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          role: frontend
		  

before applying this yaml describe networkpolicies to check rule

kubectl describe networkpolicies backend-policy

kubectl apply -f np-pod-label-allow.yml

kubectl describe networkpolicies backend-policy


kubectl run --rm -it --image=busybox --labels role=frontend net-policy
wget -qO- -T3 http://backend
exit


Lab 5. Application Lifecycle Management replicaset, deployment,
Rolling Updates, and Rollbacks, HPA


Task 1: Write a Deployment YAML and Apply it

wget https://hpe-content.s3.ap-south-1.amazonaws.com/dep-nginx.yaml
kubectl apply -f dep-nginx.yaml
kubectl get deployments
kubectl get rs
kubectl get pods

kubectl exec -it <replace pod_name> -- /bin/bash
nginx -v
exit

Task 2: Update the Deployment with a newer image

kubectl set image deployment/nginx-dep nginx-ctr=nginx:1.11 --record
kubectl describe deployments
kubectl get pods

kubectl exec -it <replace pod_name> -- /bin/bash
nginx -v
exit

Task 3: Rollback of Deployment 

kubectl rollout history deployment/nginx-dep
kubectl rollout history deployment nginx-dep --revision 1   #describe history
kubectl rollout undo deployment/nginx-dep --to-revision=1
kubectl get pods

kubectl exec -it <replace pod_name> -- /bin/bash
nginx -v
exit

Task 4: Scaling of Deployments

kubectl get deployments
kubectl get pods

kubectl scale deployment nginx-dep --replicas=8
kubectl get deployments
kubectl get pods

kubectl scale deployment nginx-dep --replicas=2
kubectl get deployments
kubectl get pods


Task 5: Deployment Auto Scaling HPA - Horizontal Pod Autoscaling

kubectl autoscale deployment nginx-dep --min=3 --max=8 --cpu-percent=70
kubectl get hpa

Lab 6: DaemonSet in Kubernetes

wget https://hpe-content.s3.ap-south-1.amazonaws.com/ds-pod.yaml

kubectl apply -f ds-pod.yaml
kubectl get ds fluent-ds
kubectl get pods -o wide
kubectl delete -f ds-pod.yaml


Lab: 7 Scheduling

Task 1: Deploy pod in specific node based on node-name / hostname

vi node-name.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodename
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeName: {node-name / hostname }

kubectl create -f node-name.yaml
kubectl get pods -o wide 
see your pod in defined node
 

Task 2: Node labeling and constraining pods

list node labels
kubectl get nodes --show-labels

kubectl label nodes <node_name> disktype=ssd # creating label for node
kubectl label nodes <node_name> disktyp- # deleting label for node

list you nodes to check labels 
kubectl get nodes --show-labels | grep "disktype=ssd"

vi nlns-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nlns-nginx-pod
  labels:
    env: test
spec:
  containers:
  - name: nlns-nginx-ctr
    image: nginx
  nodeSelector:
    disktype: ssd


kubectl apply -f nlns-pod.yaml
kubectl get pods -o wide

### Advanced Pod Scheduling

Task 3: Node Affinity
----------------------
in prefered rule initially pod scheduled to label node if none of node doesnt have any label then pod scheduled to none of label node.its desnt depends on label.
--------------------------------------------
vi node-aff-pref.yaml

apiVersion: v1
kind: Pod
metadata:
  name: na-nginx-pod1
spec:
  containers:
  - name: na-nginx-ctr1
    image: nginx
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

kubectl apply -f node-aff-pref.yaml
kubectl get pods -o wide

Remove node label 

kubectl label nodes <node_name> disktype-
deploy same pod again it is getting deployed

Task 4: required during scheduling

remove label first 
kubectl label nodes <node_name> disktype-


vi aff-na-pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: na-nginx-pod2
spec:
  containers:
  - name: na-nginx-ctr2
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd

kubectl create -f aff-na-pod2.yaml

pod is strictly looking for labeled node

kubectl get pods -o wide
kubectl label nodes <node_name> disktype=ssd

once label is added pod will deployed

Lab: 7.1  

Task 1: Pod Affinity

vi depend-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pa-nginx-pod1
  labels:
    app: pa-nginx-app
spec:
  containers:
  - name: pa-nginx-ctr1
    image: nginx

kubectl create -f depend-pod.yaml
kubectl get pods

Task 2: create pod with pod affinity

vi aff-pa-pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pa-nginx-pod2
spec:
  containers:
  - name: pa-nginx-ctr2
    image: nginx
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - pa-nginx-app
        topologyKey: kubernetes.io/hostname

kubectl create -f pod-affinity.yaml

kubectl get pods -o wide
both pod are in same node

Task 3: pod anti affinity

vi pod-antiaff.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - pa-nginx-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: nginx
        image: nginx

kubectl create -f  pod-antiaff.yaml
kubectl get pod -o wide 


kubectl delete -f  pod-antiaff.yaml


Lab: 8 Taint and tolerations:
---------------------------------

Task 2: Taint and tolerations

taint node

kubectl taint node node-name key=value:effect
kubectl taint node node-name data=test:NoExecute

cmnds:
---------

 kubectl taint node node01 data=test:NoExecute ---> for taint node with noexecute permisions,pods are not running inthis node during this stage
 kubectl taint node node01 data=test:NoSchedule ---> for taint the node with noschedule permision
 kubectl taint node node01 data-  --> delete the taint later pods can be schdules
 kubectl cordon node01  --> scheduling option is disbleling that means pods cant be scheduling
 kubectl uncordon node01 --> deleting the disable option of scheduling that means pods schedules
 kubectl describe node controlplane ---> more info about master node/control plane
 kubectl describe node node01 ---> more info about worker node
 kubectl cordon controlplane ---> schedule option disable in controlplane node
 kubectl uncordon controlplane ---> remove scheduling disable option
 
 here whatever we did by cordon,taint for master and worker those are all we can able to see in 'describe' command.


NoSchedule, PreferNoSchedule, NoExecute

vi tpod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: con1
    image: nginx:latest
  tolerations:
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
#    value: "test"
    effect: "NoSchedule"
	
  
  
  kubectl create -f tpod.yaml
  
  kubectl get pods -o wide
  your pod is in master node




Lab 9: Resource Quotas in Kubernetes

Task 1: Creating a Namespace
kubectl create namespace quotas
kubectl get ns
kubectl describe ns quotas

Task 2: Creating a resourcequota

vi rq-quotas.yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota
  namespace: quotas
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi

kubectl create -f rq-quotas.yaml
kubectl describe ns resourcequota




Task 3: Verify resourcequota Functionality

vi rq-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: quota-pod
  namespace: quotas
spec:
  containers:
  - name: quota-ctr
    image: nginx
    resources:
      limits:
        memory: "800Mi"
        cpu: "1000m"
      requests:
        memory: "600Mi"
        cpu: "350m"
    ports:
      - containerPort: 80
	  
kubectl create -f rq-pod.yaml
kubectl describe ns resourcequota
kubectl get resourcequota -n quotas -o yaml


Task 4: Limiting Number of Pods

kubectl edit resourcequotas quota -n quotas
add following lines in spec: > hard:
count/deployments.apps: 2
count/replicasets.apps: 3

Task 5: Clean-up
1.	Delete the quota to clean up.
$ kubectl delete ns quotas


Lab: 10 Volumes 


EmptyDir , hostPath , persistant volume

Task: 1 Empty Dir Volume

vi emp-vol-dep.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: emp-vol-dep
spec:
  replicas: 2
  selector:
    matchLabels:
      app: emp-app
  template:
    metadata:
      labels:
        app: emp-app
    spec:
      containers:
      - name: con1
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: myvol
          mountPath: /data
      - name: con2
        image: tomcat:latest
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: myvol
          mountPath: /data
      volumes:
      - name: myvol
        emptyDir: {}

# save file  Press ESc and :wq! to save and exit vi
kubectl create -f emp-vol-dep.yaml
kubectl get deployment


kubectl exec -it <pod-name> -c con1 -- bash
cd /data
create a  file
touch mango.txt
exit

now login to second container and check you mango file

kubectl exec -it <pod-name> -c con1 -- bash
cd /data
ls


Task: 2 hostPath volume

vi hostpath-vol.yaml

apiVersion: v1
kind: Pod
metadata:
  name: hostvol-pod
spec:
  containers:
  - name: ng
    image: nginx:latest
    ports:
    - containerPort: 80
    volumeMounts:
    - name: empvol
      mountPath: /data
  - name: tom
    image: tomcat:latest
    ports:
    - containerPort: 8080
    volumeMounts:
    - name: empvol
      mountPath: /data
  volumes:
  - name: empvol
    hostPath:
      path: /data 

kubectl apply -f hostpath-vol.yaml

Task 2.1 Login to 1st  container and create following files
kubectl exec -it hostvol-pod -c ng -- bash

cd /data/
touch file1.txt file2.txt file3.txt
ls
file1.txt  file2.txt  file3.txt
exit

Now loginto second container and verify data is accessible here also in second container.

kubectl exec -it hostvol-pod -c tom -- bash
cd /data/
ls
file1.txt  file2.txt  file3.txt
exit


Task 2.2: login into worker node where pod is deployed and check data directory created by hostPath

Ssh into worker node
Ssh ubuntu@<nodes public IP here>
Goto to path created by hostpath (/data/) and your data is here even after pod deletion.
cd /data/
/data$ ls
file1.txt  file2.txt  file3.txt
exit


: Persistent Volume in Kubernetes using storage-class
Task 3: create pvc persistant volume claim

vi pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-claim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi

# save file  Press ESc and :wq! to save and exit vi
kubectl create -f pvc.yaml
kubectl get pvc

Task 2: Create nginx Pod with persistent volume claim 

vi pvc-pod.yaml

kind: Pod
apiVersion: v1
metadata:
  name: pv-pod
spec:
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
       claimName: pv-claim
  containers:
    - name: pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: pv-storage



# Press ESc and :wq! to save and exit vi

kubectl apply -f pvc-pod.yaml
kubectl get pvc
kubectl exec -it pv-container -- bash
cd /usr/share/nginx/html
create a  file 
touch mango.txt
exit

delete pod 
kubectl delete -f pvc-pod.yaml
re-create pod again
kubectl create -f pvc-pod.yaml
check your mango.txt file is already there
kubectl exec -it pv-container -- bash
cd /mnt
ls

	
Lab 11: ConfigMap
Task 1: Setting container environment variables using configmap

imperative way to create config-map in environment variables
kubectl create configmap my-configmap --from-literal mydata=hello_world --dry-run=client -o yaml

vi config-map.yaml

apiVersion: v1
data:
  mydata: hello_world
kind: ConfigMap
metadata:
  name: my-configmap

kubectl create -f config-map.yaml
 kubectl get configmaps


vi cm-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: cm-pod
spec:
  containers:
  - name: quota-ctr
    image: nginx:latest
    ports:
    - containerPort: 80
    env:
    - name: hlo
      valueFrom:
        configMapKeyRef:
           name: my-configmap
           key: mydata
		   
kubectl exec -it cm-pod -- bash
echo $hlo
hello_world
exit


Task 2: Setting configuration file with volume using ConfigMap

vi redis-cm.yaml
 
apiVersion: v1
data:
  redis-config: |
    maxmemory 2mb
    maxmemory-policy allkeys-lru
kind: ConfigMap
metadata:
  name: example-redis-config
  namespace: default

kubectl create -f redis-cm.yaml 
kubectl get cm
kubectl describe cm example-redis-config 

Create pod

vi redis-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    env:
    - name: MASTER
      value: "true"
    ports:
    - containerPort: 6379
    volumeMounts:
    - mountPath: /redis-master-data
      name: data
    - mountPath: /redis-master
      name: config
  volumes:
    - name: data
      emptyDir: {}
    - name: config
      configMap:
        name: example-redis-config
        items:
        - key: redis-config
          path: redis.conf

kubectl create -f redis-pod.yaml
read config map's file 
kubectl exec -it redis cat /redis-master/redis.conf

Task 3: Creating a Secret

vi secrets.yaml

apiVersion: v1
kind: Secret
metadata:
  name: mysql-credentials
type: Opaque
data:
  ##all below values are base64 encoded
  ##rootpw is root
  ##user is user
  ##password is mypwd
  rootpw: cm9vdAo=
  user: dXNlcgo=
  password: bXlwd2QK
  
kubectl create -f secrets.yaml
kubectl get secrets
kubectl describe secrets mysql-credentials
see your encoded secrets 
kubectl get secrets mysql-credentials -o yaml


